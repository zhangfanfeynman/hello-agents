import os
import sys
from typing import Optional
from openai import OpenAI
from hello_agents import HelloAgentsLLM

print("=" * 60, flush=True)
print("ğŸš€ å¼€å§‹æµ‹è¯• Ollama è¿æ¥...", flush=True)
print("=" * 60, flush=True)

try:
    llm_client = HelloAgentsLLM(
        provider="ollama",
        model="llama3", # éœ€ä¸ `ollama run` æŒ‡å®šçš„æ¨¡å‹ä¸€è‡´
        base_url="http://localhost:11434/v1",
        api_key="ollama" # æœ¬åœ°æœåŠ¡åŒæ ·ä¸éœ€è¦çœŸå® Key
    )
    
    print("\nâœ… LLM å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ", flush=True)
    print(f"ğŸ“‹ Provider: {llm_client.provider}", flush=True)
    print(f"ğŸ“‹ Model: {llm_client.model}", flush=True)
    print(f"ğŸ“‹ Base URL: {llm_client.base_url}", flush=True)
    
    # å‡†å¤‡æ¶ˆæ¯
    messages = [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚"}]
    
    print("\nğŸ”„ æ­£åœ¨è°ƒç”¨ Ollama API...\n", flush=True)
    
    # å‘èµ·è°ƒç”¨ï¼Œthinkç­‰æ–¹æ³•éƒ½å·²ä»çˆ¶ç±»ç»§æ‰¿ï¼Œæ— éœ€é‡å†™
    response_stream = llm_client.think(messages)
    
    # æ‰“å°å“åº”
    print("\nğŸ“ Ollama Response:", flush=True)
    print("-" * 60, flush=True)
    
    response_text = ""
    for chunk in response_stream:
        # chunk å·²ç»æ˜¯æ–‡æœ¬ç‰‡æ®µï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨
        print(chunk, end="", flush=True)
        response_text += chunk
    
    print("\n" + "-" * 60, flush=True)
    print(f"\nâœ… å®Œæˆï¼å…±æ¥æ”¶ {len(response_text)} ä¸ªå­—ç¬¦", flush=True)
    print("=" * 60, flush=True)
    
except Exception as e:
    print(f"\nâŒ é”™è¯¯: {e}", flush=True)
    import traceback
    traceback.print_exc()
    sys.exit(1)